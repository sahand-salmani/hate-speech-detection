{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bc3eae1-03dc-4ec4-b5a6-7656c4ff5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec45e30e-680a-445e-9b2b-0e13c20c2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/HateSpeechDataset.csv', usecols=['Content', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c866fce-fb47-4fe4-a0ab-ed7a32707c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content Label\n",
       "0  denial of normal the con be asked to comment o...     1\n",
       "1  just by being able to tweet this insufferable ...     1\n",
       "2  that is retarded you too cute to be single tha...     1\n",
       "3  thought of a real badass mongol style declarat...     1\n",
       "4                                afro american basho     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03c728-069b-4a9f-8e64-93eeef4ce9dd",
   "metadata": {},
   "source": [
    "### Lowering the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe364e5f-3904-4bea-bf10-08b93f3a6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowered_collumns_name = [i.lower() for i in df.columns]\n",
    "df.columns = lowered_collumns_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748eeb67-ed6a-49a9-9d93-e657288855d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23345 row of duplicated value droped\n"
     ]
    }
   ],
   "source": [
    "duplicated_rows_count = df['content'].duplicated().sum()\n",
    "if duplicated_rows_count > 0:\n",
    "    df = df.drop_duplicates(subset='content', keep='first')\n",
    "    print(f'{duplicated_rows_count} row of duplicated value droped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f01bf54-2f39-45a2-b420-958d7086918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = [\n",
    "    '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "    ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
    "]\n",
    "pattern = r'\\b[a-zA-Z]+\\b'\n",
    "def remove_special_chars_from_text(text):\n",
    "    temp = ''.join([i if i not in special_characters else ' ' for i in str(text)])\n",
    "    return ' '.join(re.findall(pattern, temp))\n",
    "\n",
    "def remove_multiple_spaces(s):\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "class Text_Util:\n",
    "    @staticmethod\n",
    "    def lower_text_case(text):\n",
    "        if not isinstance(text, np.ndarray):\n",
    "            raise Exception('entered an invalid dtype for the lower_text_case')\n",
    "\n",
    "        if text.dtype.type is not np.str_:\n",
    "            text = text.astype(str)\n",
    "        return np.char.lower(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def special_chars_remover(text):\n",
    "        if not isinstance(text, np.ndarray):\n",
    "            raise Exception('entered an invalid dtype for the remove_special_chars_from_text')\n",
    "        special_chars_remover_util = np.vectorize(remove_special_chars_from_text)\n",
    "        return special_chars_remover_util(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def extra_white_space_remover(text):\n",
    "        if not isinstance(text, np.ndarray):\n",
    "            raise Exception('entered an invalid dtype for the extra_white_space_remover')\n",
    "        vectorized_remove_spaces = np.vectorize(remove_multiple_spaces)\n",
    "        return vectorized_remove_spaces(text)\n",
    "\n",
    "    \n",
    "\n",
    "class Text_Preprossesor:\n",
    "    def __init__(self):\n",
    "        self.content = []\n",
    "        self.label_names = []\n",
    "        self.pipe_line = []\n",
    "        self.count_vectorizer = None\n",
    "\n",
    "    def collect_utils(self):\n",
    "        self.pipe_line.append(Text_Util.lower_text_case)\n",
    "        self.pipe_line.append(Text_Util.special_chars_remover)\n",
    "        self.pipe_line.append(Text_Util.extra_white_space_remover)\n",
    "\n",
    "    def process_text(self, features):\n",
    "        for func in self.pipe_line:\n",
    "            features = func(features)\n",
    "        return features\n",
    "\n",
    "    def count_vectorizer_initialize(self, word_count = 10000):\n",
    "        self.count_vectorizer = CountVectorizer(max_features=word_count)\n",
    "\n",
    "    def vectorize_contet_fit(self):\n",
    "        self.count_vectorizer.fit(self.content)\n",
    "        self.label_names = self.count_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        \n",
    "    def fit(self, text, word_count = 10000):\n",
    "        if isinstance(text, str):\n",
    "            self.content = np.array([text])\n",
    "        elif isinstance(text, np.ndarray):\n",
    "            self.content = text\n",
    "        elif isinstance(text, pd.Series):\n",
    "            self.content = text.values\n",
    "        else:\n",
    "            raise Exception('entered an invalid dtype for the text processing')\n",
    "\n",
    "        self.collect_utils()\n",
    "        self.content = self.process_text(self.content)\n",
    "        self.count_vectorizer_initialize(word_count)\n",
    "        self.vectorize_contet_fit()\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.count_vectorizer.transform(self.content)\n",
    "\n",
    "    def get_label_names(self):\n",
    "        return self.label_names\n",
    "\n",
    "    def vectorize_text(self, text):\n",
    "        processed_text = []\n",
    "        if isinstance(text, str):\n",
    "            processed_text = np.array([text])\n",
    "        elif isinstance(text, np.ndarray):\n",
    "            processed_text = text\n",
    "        elif isinstance(text, pd.Series):\n",
    "            processed_text = text.values\n",
    "        else:\n",
    "            raise Exception('entered an invalid dtype for the text processing')\n",
    "\n",
    "        return self.count_vectorizer.transform(processed_text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7f6f1d-1118-4088-a728-0add740015a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = Text_Preprossesor()\n",
    "text_processor.fit(df.iloc[:, 0])\n",
    "X = text_processor.get_features()\n",
    "y = df.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8b3d3e-0792-4de8-bd99-caf886cb30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd4494f5-eeac-4523-b4fc-d3699a3bc293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1500)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1500)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab95f9aa-7bfd-4f39-96ba-3b5828ec68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.0%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     68191\n",
      "           1       0.69      0.46      0.55     15322\n",
      "\n",
      "    accuracy                           0.86     83513\n",
      "   macro avg       0.79      0.71      0.74     83513\n",
      "weighted avg       0.85      0.86      0.85     83513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "cls_report = classification_report(y_test, predictions)\n",
    "print(f\"Accuracy: {round(accuracy, 2) * 100}%\")\n",
    "print(cls_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
